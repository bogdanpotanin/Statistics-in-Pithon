# --------
# Потанин Богдан Станиславович
# Математическая Статистика в Python
# Урок 8. Метод максимального правдоподобия
# --------

import sys
import math
import pandas as pd
import numpy as np
import scipy
import matplotlib.pyplot as plt
import seaborn
from scipy.stats import t, chi2, f, norm, poisson, \
     binom, uniform, multivariate_normal, expon
from scipy.optimize import approx_fprime, minimize, line_search

np.set_printoptions(suppress = True)                    # уберем scientific notation

# --------
# Часть №1. Получение оценок методом
#           максимального правдоподобия
# --------

# Сгенерируем выборку из экспоненциального
# распределения
np.random.seed(123)                                     # для воспроизводимости результатов, полученных с использованием
                                                        # выборок, необходима установка случайного зерна
n = 1000                                                # объем выборки
Lambda = 0.2                                            # параметр экспоненциального распределения
x = expon.rvs(size = n, scale = 1 / Lambda)             # реализация выборки
# Обратите внимание, что во всех функция, связанных
# с expon, параметр распределения соотносится
# со scale как 1 / lambda, а не просто lambda

# Используя метод максимального правдоподобия
# находим оценку параметра аналитически
Lambda_est_1 = 1 / np.mean(x)

# Также, оценку можно найти при помощи
# численных методов оптимизации.
# Осуществим оценивание в два этапа.

# 1. Запрограммируем логарифм функции
#    правдоподобия
def lnL(Lambda,                                         # параметр экспоненциального распределения
        x,                                              # реализация выборки
        scale = 1):                                     # параметр, позволяющий домножить логарифм функции
                                                        # правдоподобия на scale
    f_x = expon.pdf(x, scale = 1 / Lambda)              # считаем функцию плотности для каждого наблюдения
    lnL_value = np.log(f_x)                             # и логарифмируем каждую из них

    return sum(lnL_value) * scale                       # суммируем посчитанные логарифмы и домножаем
                                                        # полученный результат на scale
# Посчитаем значение логарифма функции
# правдоподобия
lnL(Lambda, x)                                          # при истинном значении параметра
lnL(Lambda_est_1, x)                                    # в точке реализации ММП оценки параметра
lnL(0.1, x, -1)                                         # в некоторой произвольной точке, домножая результат на -1

# 2. Максимизируем значение запрограммированной
#    функции при помощи численного метода оптимизации,
#    который ищет минимум функции при попомощи
#    'умного' перебора различных значений
#    оптимизируемых параметров, в данном случае Lambda.
#    Для того, чтобы найти не минимум, а максимум
#    функции, достаточно умножить ее на -1.
lnL_opt = minimize(fun = lnL,                           # максимизируемая функция
                   x0 = 0.1,                            # начальная точка, из которой стартует оптимизация: можно взять
                                                        # произвольную, лишь бы оптимизируемая функция была в ней определена
                   args = (x, -1),                      # значения дополнительных, то есть не оптимизируемых параметров
                                                        # функции правдоподобия, то есть (x, scale), где scale = -1
                                                        # позволяет превратить минимизационную задачу в максимизационную.
                                                        # В качестве оптимизируемого параметра minimize автоматически выбирает
                                                        # первый из параметров функции lnL, в данном случае Lambda.
                   method='BFGS')                       # численный метод оптимизации
Lambda_est_2 = lnL_opt['x']                             # достаем оценку как найденный численным методом максимум
                                                        # функции правдоподобия
print([Lambda_est_1, Lambda_est_2])                     # убеждаемся, что полученные двумя способами значения ММП
                                                        # оказались практичеки идентичными: с точностью до погрешности,
                                                        # обусловленной ограничениями вычислительными способностями компьютера

# --------
# Часть №2. Информация фишера и
#           асимптотическая дисперсия
# --------

# Аналитически находим
I = n / Lambda ** 2                                     # информацию Фишера
Lambda_var = 1 / I                                      # асимптотическую дисперсию ММП оценки

# Теперь, используя инвариантность
# ММП оценок найдем ММП оценки
I_est = n / Lambda_est_1 ** 2                           # информации Фишера
Lambda_var_est = 1 / I_est                              # асимптотической дисперсии ММП оценки

# --------
# Часть №3. Асимптотическая нормальность
#           ММП оценок
# --------

# Сгенерируем выборку из ММП оценок, используя
# несколько выборок из экспоненциального распределения
np.random.seed(123)
m = 10000                                               # генерируем идентичные выборки
Lambda_est_vec = np.empty(m)                            # для каждой из которых будут рассчитаны оценки, то есть
                                                        # в итоге будет получена выборка из ММП оценок
for i in range(0, m):
    x_new = expon.rvs(size = n, scale = 1 / Lambda)     # генерируем новую выборку
    Lambda_est_vec[i] = 1 / np.mean(x_new)              # считаем по ней оценку
Lambda_est_vec = np.sort(Lambda_est_vec)                # отсуртируем полученную выборку для удобства
                                                        # построения графиков

# Оценим некоторые характеристики
# распределения ММП оценок по полученной
# выборке из данных оценок
np.mean(Lambda_est_vec)                                 # выборочные оценки математического ожидания и дисперсии
np.var(Lambda_est_vec)                                  # ММП оценок ожидаемо близки как к истинным значениям так и к
                                                        # оценкам, полученным выше
np.quantile(Lambda_est_vec, 0.9)                        # по аналогии можно оценить и иные характеристики распределения
                                                        # ММП оценок, например, квантиль уровня 0.9

# Рассчитаем вероятность того, что оценка
# не превысит 0.205, используя асимптотическую
# нормальность ММП оценок
norm.cdf(x = 0.205,                                     # приблизительная истинная вероятность, рассчитанная
         loc = Lambda,                                  # при допущении о том, что ММП оценка имеет нормальное
         scale = np.sqrt(Lambda_var))                   # распределение: на самом деле распределение, обычно,
                                                        # будет не в точности нормальным, но крайне близким к
                                                        # нему при достаточно большом объеме выборки
norm.cdf(x = 0.205,                                     # оценка приблизительной вероятности, полученная с помощью
         loc = Lambda_est_1,                            # свойства инвариантности ММП оценок
         scale = np.sqrt(Lambda_var_est))
np.mean(Lambda_est_vec <= 0.205)                        # оценка вероятности не приблизительной, а точной, учитывающей
                                                        # настоящее, а не асимптотическое распределение, полученная
                                                        # с помощью выборочной функции распределения, сконуструированной
                                                        # по выборке из ММП оценок
# Отметим, что некоторые или даже все характеристики
# истинного и асимптотического распределений,
# например, дисперсия, иногда могут совпадать

# Визуализируем асимптотическую нормальность при
# помощи гистограммы и оценки графика асимптотической
# функции плотности рассматриваемой ММП оценки
f_Lambda = norm.pdf(Lambda_est_vec,                     # рассчитываем значения асимптотической функции плотности
                        loc = Lambda,                   # ММП оценки лямбды в точках, определяемых реализациями
                        scale = np.sqrt(Lambda_var))    # этих оценок
f_Lambda_est = norm.pdf(Lambda_est_vec,                 # по аналогии рассчитываем оценки асимптотической функции
                        loc = Lambda_est_1,             # плотности ММП оценки: заменяем истинные параметры распределения
                        scale = np.sqrt(Lambda_var_est))# ММП оценки лямбды на их оценки
plt.xlabel('x')                                         # название оси x
plt.ylabel('F(x)')                                      # название оси y
plt.plot(Lambda_est_vec,                                # строим графики
         f_Lambda_est, '--',
         linewidth = 3,
         label = "Asymptotic PDF",
         color = "limegreen")
plt.plot(Lambda_est_vec,
         f_Lambda, '--',
         linewidth = 3,
         label = "Asymptotic PDF Estimate",
         color = "indianred")
seaborn.histplot(Lambda_est_vec,                        # гистограмма
                 stat = 'density',
                 color = "palevioletred",
                 label = "histogram",
                 bins = 35)
plt.legend()
# Воспроизведите данный эксперимент при n = 10 (не m) и
# убедитесь, что при столь малом объеме выборки
# рассматриваемые графики не будут похожи, поскольку
# большинство свойств ММП оценок является асимптотическим,
# потому результат их действия, включая асимптотическую
# нормальность, можно наблюдать лишь на выборка
# достаточно большого объема

# --------
# Часть №4. Асимптотическая нормальность
#           функций от ММП оценок
# --------

# Рассмотрим распределение оценки информации
# Фишера, обратив внимание, что она является
# функцией от ММП оценки параметра 'lambda'.
# Следовательно, асимптотическое распределеине
# оценки информации Фишера будет нормальным со
# следующими, получаеыми по дельта методу
# параметрами:
I_mean = I                                              # асимптотическое математическое ожидание
I_var = Lambda_var * (2 * n / (Lambda ** 3)) ** 2       # асимптотическая дисперсия
I_var_est = Lambda_var_est * (2 * n /                   # и ее ММП оценка
                              (Lambda_est_1 ** 3)) ** 2

# Рассмотрим выборку из ММП оценок информаци Фишера
I_est_vec = n / (Lambda_est_vec ** 2)                   # соответствующая выборка
np.var(I_est_vec)                                       # выборочная оценка дисперсии

# Визуализируем асимптотическую нормальность при
# помощи гистограммы и оценки графика асимптотической
# функции плотности рассматриваемой ММП оценки
f_Lambda = norm.pdf(I_est_vec,                          # рассчитываем значения асимптотической функции плотности
                        loc = I,                        # ММП оценки информации Фишера в каждой точке реализации
                        scale = np.sqrt(I_var))         # соответствующих ММП оценок
f_Lambda_est = norm.pdf(I_est_vec,                      # рассчитываем оценки асимптотической функции плотности
                        loc = I_est,                    # ММП оценки в каждой точке реализации ММП оценок
                        scale = np.sqrt(I_var_est))     # соответствующих ММП оценок
plt.xlabel('x')                                         # название оси x
plt.ylabel('F(x)')                                      # название оси y
plt.plot(I_est_vec,                                     # настоящая асимптотическая плотность ММП оценки информации Фишера
         f_Lambda_est, '--',
         linewidth = 3,
         label = "Asymptotic PDF",
         color = "limegreen")
plt.plot(I_est_vec,                                     # оценка асимптотической плотности ММП оценки информции Фишера
         f_Lambda, '--',
         linewidth = 3,
         label = "Asymptotic PDF Estimate",
         color = "indianred")
seaborn.histplot(I_est_vec,                             # гистограмма
                 stat = 'density',
                 color = "palevioletred",
                 label = "histogram",
                 bins = 35)
plt.legend()

# Задания
# 1. По выборке из экспоненциального распределения
#    с параметром 'lambda = 0.3' при объемах выборки
#    n равных 10, 20, 50, 100 и 1000 сравните, как
#    соотносятся:
#    1)     Истинное значение параметра и его ММП оценка
#    2)     Истинное значение асимптотической дисперсии
#           ММП оценки и ее (этой дисперсии) ММП оценка
#    3)     Асимптотическая вероятность того, что
#           параметр отклонится от истинного значения
#           более, чем на 2%, и ММП оценка этой вероятности
#    4*)    Сравните полученные в предыдущем пункте
#           значения с как можно более точной, то есть
#           полученной по очень большой выборке из ММП
#           оценок, оценкой истинной вероятности того, что
#           ММП оценка отклонится от истинного значения
#           оцениваемого параметра более, чем на 2%
#    5)     График функции плотности асимптотического распределения
#           ММП оценки и его оценка
#    6*)    Повторите предыдущие пункты для ММП оценки
#           асимптотической дисперсии ММП оценки параметра 'lambda'
#    7**)   Повторите предыдущие пункты для ММП оценки вероятности
#           из пункта 3)
#    8)     Повторите предыдущие пункты для распределения
#           Пуассона с параметром 'lambda = 5'
# 2. Используя численный метод оптимизации по выборке из 1000
#    наблюдений найдите ММП оценки параметров следующих распределений:
#    1)     Распределение Хи-квадрат с 5-ю степенями свободы
#    2)     Распределение Стьюдента с 10-ю степенями свободы
#    3**)   Гамма распределение с параметрами (1, 1)